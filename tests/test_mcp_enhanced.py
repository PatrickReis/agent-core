#!/usr/bin/env python3
"""
Script de teste para as funcionalidades do MCP Server Enhanced.
Demonstra como usar todas as funcionalidades dispon√≠veis.
"""

import asyncio
import json
from typing import Dict, List, Any

# Simular cliente MCP para testes
class MCPTestClient:
    """Cliente de teste para simular chamadas MCP."""

    def __init__(self, server_url: str = "local"):
        self.server_url = server_url
        self.session_id = "test_session"

    async def call_tool(self, tool_name: str, arguments: Dict[str, Any]) -> Dict[str, Any]:
        """Simula chamada de tool MCP."""
        print(f"üì° Chamando tool: {tool_name}")
        print(f"üìã Argumentos: {json.dumps(arguments, indent=2, ensure_ascii=False)}")

        # Aqui voc√™ conectaria ao servidor MCP real
        # Por enquanto, retornamos dados simulados
        return {"result": "simulado", "tool": tool_name, "args": arguments}

    async def get_resource(self, resource_uri: str) -> Dict[str, Any]:
        """Simula obten√ß√£o de resource MCP."""
        print(f"üì¶ Obtendo resource: {resource_uri}")
        return {"resource": resource_uri, "data": "simulado"}


async def test_evaluation_system():
    """Testa o sistema de avalia√ß√£o de prompts."""
    print("\nüß™ === TESTANDO SISTEMA DE AVALIA√á√ÉO ===")

    client = MCPTestClient()

    # Teste 1: Avalia√ß√£o r√°pida
    test_prompts = [
        "Qual √© a temperatura em S√£o Paulo?",
        "Explique o que √© Python",
        "Como est√° o clima hoje?",
        "O que significa IA?"
    ]

    print("\n1Ô∏è‚É£ Teste de Avalia√ß√£o R√°pida:")
    result = await client.call_tool("run_agent_evaluation", {
        "test_prompts": test_prompts,
        "suite_name": "quick_test"
    })
    print(f"‚úÖ Resultado: {json.dumps(result, indent=2, ensure_ascii=False)}")


async def test_advanced_reasoning():
    """Testa o sistema de reasoning avan√ßado."""
    print("\nüß† === TESTANDO REASONING AVAN√áADO ===")

    client = MCPTestClient()

    # Teste com diferentes n√≠veis de complexidade
    test_questions = [
        {
            "question": "Por que o c√©u √© azul?",
            "complexity": "simple"
        },
        {
            "question": "Como funciona o machine learning e qual sua aplica√ß√£o em sistemas de recomenda√ß√£o?",
            "complexity": "complex"
        },
        {
            "question": "Analise os pr√≥s e contras da implementa√ß√£o de microservi√ßos vs arquitetura monol√≠tica",
            "complexity": "advanced"
        }
    ]

    for i, test in enumerate(test_questions, 1):
        print(f"\n{i}Ô∏è‚É£ Teste de Reasoning - {test['complexity'].upper()}:")
        print(f"Pergunta: {test['question']}")

        result = await client.call_tool("enhanced_reasoning", {
            "question": test["question"],
            "complexity_override": test["complexity"]
        })
        print(f"‚úÖ Resultado: {json.dumps(result, indent=2, ensure_ascii=False)[:200]}...")


async def test_feature_management():
    """Testa o gerenciamento de features."""
    print("\n‚öôÔ∏è === TESTANDO GERENCIAMENTO DE FEATURES ===")

    client = MCPTestClient()

    # Listar features habilitadas
    print("\n1Ô∏è‚É£ Listando features habilitadas:")
    result = await client.call_tool("manage_features", {"action": "list"})
    print(f"‚úÖ Features: {json.dumps(result, indent=2, ensure_ascii=False)}")

    # Ver status completo
    print("\n2Ô∏è‚É£ Status completo das features:")
    result = await client.call_tool("manage_features", {"action": "status"})
    print(f"‚úÖ Status: {json.dumps(result, indent=2, ensure_ascii=False)}")

    # Teste de habilita√ß√£o/desabilita√ß√£o (apenas simulado)
    print("\n3Ô∏è‚É£ Simulando toggle de feature:")
    print("‚ùó Em produ√ß√£o, use com cuidado - pode afetar funcionalidades")


async def test_resources():
    """Testa os resources do sistema."""
    print("\nüìä === TESTANDO RESOURCES ===")

    client = MCPTestClient()

    # Configura√ß√£o do sistema
    print("\n1Ô∏è‚É£ Configura√ß√£o Enhanced:")
    config = await client.get_resource("config://enhanced")
    print(f"‚öôÔ∏è Config: {json.dumps(config, indent=2, ensure_ascii=False)}")

    # M√©tricas do sistema
    print("\n2Ô∏è‚É£ M√©tricas do Sistema:")
    metrics = await client.get_resource("metrics://system")
    print(f"üìà M√©tricas: {json.dumps(metrics, indent=2, ensure_ascii=False)}")

    # Health check
    print("\n3Ô∏è‚É£ Health Check:")
    health = await client.get_resource("health://check")
    print(f"üè• Sa√∫de: {json.dumps(health, indent=2, ensure_ascii=False)}")


async def test_reasoning_traces():
    """Testa obten√ß√£o de traces de reasoning."""
    print("\nüîç === TESTANDO REASONING TRACES ===")

    client = MCPTestClient()

    # Primeiro, executar reasoning para gerar trace
    print("1Ô∏è‚É£ Gerando trace de reasoning:")
    reasoning_result = await client.call_tool("enhanced_reasoning", {
        "question": "Qual a diferen√ßa entre Python e JavaScript?"
    })

    if "trace_id" in reasoning_result.get("result", {}):
        trace_id = reasoning_result["result"]["trace_id"]

        # Obter trace detalhado
        print(f"\n2Ô∏è‚É£ Obtendo trace detalhado: {trace_id}")
        trace_result = await client.call_tool("get_reasoning_trace", {
            "trace_id": trace_id
        })
        print(f"üîç Trace: {json.dumps(trace_result, indent=2, ensure_ascii=False)}")
    else:
        print("‚ùå N√£o foi poss√≠vel obter trace_id")


def create_eval_examples():
    """Cria exemplos pr√°ticos de uso do sistema de evals."""
    print("\nüìù === EXEMPLOS DE USO DOS EVALS ===")

    examples = {
        "eval_basico": {
            "descricao": "Avalia√ß√£o b√°sica com poucos prompts",
            "prompts": [
                "O que √© Python?",
                "Como est√° o tempo hoje?",
                "Explique machine learning"
            ],
            "uso": "await client.call_tool('run_agent_evaluation', {'test_prompts': prompts, 'suite_name': 'basico'})"
        },

        "eval_completo": {
            "descricao": "Suite completa com casos de teste estruturados",
            "exemplo_codigo": """
# Exemplo de uso program√°tico do framework de evals
from utils.prompt_evals import EvalFramework, create_standard_eval_suite

# Criar framework
framework = EvalFramework()

# Definir casos de teste
test_cases = [
    {
        "id": "python_concept",
        "prompt": "O que √© Python?",
        "expected": "Python √© uma linguagem de programa√ß√£o",
        "tags": ["conceito", "programa√ß√£o"]
    },
    {
        "id": "weather_query",
        "prompt": "Qual a temperatura em SP?",
        "tags": ["clima", "tool_usage"]
    }
]

# Criar suite
suite = create_standard_eval_suite("minha_suite", test_cases)

# Executar com fun√ß√£o do agente
def meu_agente(prompt):
    # Sua l√≥gica aqui
    return "resposta do agente"

result = suite.run(meu_agente)
print(f"Score: {result.summary['overall_score']:.1f}%")
"""
        },

        "metricas_disponiveis": {
            "descricao": "M√©tricas avaliadas automaticamente",
            "metricas": {
                "relevance": "Relev√¢ncia da resposta ao prompt (0-100%)",
                "accuracy": "Precis√£o comparada com resposta esperada (0-100%)",
                "latency": "Tempo de resposta (baseado em target de 2s)",
                "safety": "Seguran√ßa - detecta conte√∫do perigoso (0-100%)",
                "tool_usage": "Uso apropriado de ferramentas (0-100%)"
            }
        }
    }

    for nome, exemplo in examples.items():
        print(f"\nüìã {nome.upper()}:")
        print(f"   {exemplo['descricao']}")

        if "prompts" in exemplo:
            print(f"   Prompts: {exemplo['prompts']}")
            print(f"   Uso: {exemplo['uso']}")

        if "exemplo_codigo" in exemplo:
            print(f"   C√≥digo de exemplo:")
            print(exemplo["exemplo_codigo"])

        if "metricas" in exemplo:
            print("   M√©tricas avaliadas:")
            for metrica, desc in exemplo["metricas"].items():
                print(f"   ‚Ä¢ {metrica}: {desc}")


def show_available_features():
    """Mostra todas as funcionalidades dispon√≠veis."""
    print("\nüöÄ === FUNCIONALIDADES DISPON√çVEIS ===")

    features = {
        "Tools MCP": {
            "run_agent_evaluation": "Executa avalia√ß√£o de prompts com m√∫ltiplas m√©tricas",
            "enhanced_reasoning": "Reasoning multi-step com an√°lise complexa",
            "manage_features": "Gerencia feature toggles do sistema",
            "get_reasoning_trace": "Obt√©m traces detalhados de reasoning"
        },

        "Resources MCP": {
            "config://enhanced": "Configura√ß√£o completa do servidor",
            "metrics://system": "M√©tricas b√°sicas do sistema",
            "metrics://detailed": "M√©tricas detalhadas (admin)",
            "health://check": "Health check completo"
        },

        "Prompts MCP": {
            "enhanced-agent-query": "Template para consultas ao agente",
            "evaluation-prompt": "Template para cen√°rios de teste"
        },

        "Features Habilitadas": {
            "prompt_evaluation": "Sistema de avalia√ß√£o de prompts",
            "advanced_reasoning": "Reasoning avan√ßado multi-step",
            "enhanced_logging": "Logging estruturado com m√©tricas",
            "performance_monitoring": "Monitoramento de performance"
        }
    }

    for categoria, items in features.items():
        print(f"\nüì¶ {categoria}:")
        for nome, desc in items.items():
            print(f"   üîπ {nome}: {desc}")


async def main():
    """Executa todos os testes das funcionalidades."""
    print("üéØ TESTANDO MCP SERVER ENHANCED")
    print("=" * 50)

    # Mostrar funcionalidades dispon√≠veis
    show_available_features()

    # Criar exemplos de uso
    create_eval_examples()

    # Executar testes das funcionalidades
    await test_evaluation_system()
    await test_advanced_reasoning()
    await test_feature_management()
    await test_resources()
    await test_reasoning_traces()

    print("\n" + "=" * 50)
    print("‚úÖ TESTES CONCLU√çDOS!")
    print("\nüí° PR√ìXIMOS PASSOS:")
    print("1. Conecte um cliente MCP real ao seu servidor")
    print("2. Execute as tools atrav√©s do cliente MCP")
    print("3. Monitore logs do servidor para ver execu√ß√£o real")
    print("4. Use os resources para verificar status e m√©tricas")
    print("5. Experimente diferentes cen√°rios de avalia√ß√£o")


if __name__ == "__main__":
    asyncio.run(main())