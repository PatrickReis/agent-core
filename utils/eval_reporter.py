#!/usr/bin/env python3
"""
Eval Reporter - Gerador de relat√≥rios para resultados de avalia√ß√µes.
Cria relat√≥rios em m√∫ltiplos formatos (JSON, HTML, CSV, Markdown).

Usage:
    python utils/eval_reporter.py --input results.json --format html
    python utils/eval_reporter.py --directory eval_results --format all
    python utils/eval_reporter.py --comparison comparison_results.json --output report.html
"""

import os
import sys
import json
import csv
import argparse
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Any, Optional
import statistics

# Adicionar o diret√≥rio raiz ao path
sys.path.append(str(Path(__file__).parent.parent))


class EvalReporter:
    """Gerador de relat√≥rios para resultados de avalia√ß√µes."""

    def __init__(self):
        self.templates_dir = Path(__file__).parent / "templates"
        self.templates_dir.mkdir(exist_ok=True)
        self._create_html_template()

    def generate_single_eval_report(self, result_data: Dict[str, Any],
                                   format_type: str = "html",
                                   output_file: Optional[str] = None) -> str:
        """Gera relat√≥rio para uma avalia√ß√£o individual."""

        if format_type == "html":
            return self._generate_html_single_report(result_data, output_file)
        elif format_type == "json":
            return self._generate_json_report(result_data, output_file)
        elif format_type == "csv":
            return self._generate_csv_single_report(result_data, output_file)
        elif format_type == "markdown":
            return self._generate_markdown_single_report(result_data, output_file)
        else:
            raise ValueError(f"Formato n√£o suportado: {format_type}")

    def generate_comparison_report(self, comparison_data: Dict[str, Any],
                                 format_type: str = "html",
                                 output_file: Optional[str] = None) -> str:
        """Gera relat√≥rio para compara√ß√£o entre modelos."""

        if format_type == "html":
            return self._generate_html_comparison_report(comparison_data, output_file)
        elif format_type == "json":
            return self._generate_json_report(comparison_data, output_file)
        elif format_type == "csv":
            return self._generate_csv_comparison_report(comparison_data, output_file)
        elif format_type == "markdown":
            return self._generate_markdown_comparison_report(comparison_data, output_file)
        else:
            raise ValueError(f"Formato n√£o suportado: {format_type}")

    def _generate_html_single_report(self, result_data: Dict[str, Any],
                                   output_file: Optional[str] = None) -> str:
        """Gera relat√≥rio HTML para avalia√ß√£o individual."""
        if output_file is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            output_file = f"eval_report_{timestamp}.html"

        # Extrair dados principais
        summary = result_data.get('summary', {})
        metrics = summary.get('metrics', {})
        overall_score = summary.get('overall_score', 0)
        overall_grade = summary.get('overall_grade', 'N/A')
        execution_time = summary.get('execution_time', 0)

        # Template HTML
        html_content = f"""
<!DOCTYPE html>
<html lang="pt-BR">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Relat√≥rio de Avalia√ß√£o - {summary.get('suite', 'N/A')}</title>
    <style>
        {self._get_css_styles()}
    </style>
</head>
<body>
    <div class="container">
        <header>
            <h1>üìä Relat√≥rio de Avalia√ß√£o</h1>
            <div class="header-info">
                <span>ü§ñ Agente: {summary.get('agent', 'N/A')}</span>
                <span>üìù Suite: {summary.get('suite', 'N/A')}</span>
                <span>üìÖ Data: {datetime.now().strftime('%d/%m/%Y %H:%M')}</span>
            </div>
        </header>

        <section class="overview">
            <h2>üìà Vis√£o Geral</h2>
            <div class="score-card">
                <div class="score-main">
                    <div class="score-value grade-{overall_grade.lower()}">{overall_score:.1f}%</div>
                    <div class="score-grade">Nota {overall_grade}</div>
                </div>
                <div class="score-details">
                    <div class="detail-item">
                        <span class="label">Total de Testes:</span>
                        <span class="value">{summary.get('total_tests', 0)}</span>
                    </div>
                    <div class="detail-item">
                        <span class="label">Tempo de Execu√ß√£o:</span>
                        <span class="value">{execution_time:.2f}s</span>
                    </div>
                </div>
            </div>
        </section>

        <section class="metrics">
            <h2>üéØ M√©tricas Detalhadas</h2>
            <div class="metrics-grid">
                {self._generate_metrics_html(metrics)}
            </div>
        </section>

        <section class="analysis">
            <h2>üîç An√°lise</h2>
            {self._generate_analysis_html(metrics, overall_score)}
        </section>

        <footer>
            <p>Relat√≥rio gerado em {datetime.now().strftime('%d/%m/%Y √†s %H:%M:%S')} pelo Agent Core Eval System</p>
        </footer>
    </div>
</body>
</html>"""

        # Salvar arquivo
        with open(output_file, 'w', encoding='utf-8') as f:
            f.write(html_content)

        print(f"üìÑ Relat√≥rio HTML gerado: {output_file}")
        return output_file

    def _generate_html_comparison_report(self, comparison_data: Dict[str, Any],
                                       output_file: Optional[str] = None) -> str:
        """Gera relat√≥rio HTML para compara√ß√£o entre modelos."""
        if output_file is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            output_file = f"comparison_report_{timestamp}.html"

        # Extrair dados
        models = comparison_data.get('models', [])
        suite_name = comparison_data.get('suite_name', 'N/A')
        winner = comparison_data.get('winner', 'N/A')
        individual_results = comparison_data.get('individual_results', {})
        comparison_metrics = comparison_data.get('comparison_metrics', {})

        # Preparar dados para visualiza√ß√£o
        model_scores = []
        for model in models:
            if model in individual_results:
                result = individual_results[model]
                if 'summary' in result:
                    model_scores.append({
                        'model': model,
                        'score': result['summary'].get('overall_score', 0),
                        'grade': result['summary'].get('overall_grade', 'F'),
                        'time': result['summary'].get('execution_time', 0)
                    })

        model_scores.sort(key=lambda x: x['score'], reverse=True)

        html_content = f"""
<!DOCTYPE html>
<html lang="pt-BR">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Relat√≥rio de Compara√ß√£o - {suite_name}</title>
    <style>
        {self._get_css_styles()}
        {self._get_comparison_css()}
    </style>
</head>
<body>
    <div class="container">
        <header>
            <h1>üèÜ Relat√≥rio de Compara√ß√£o de Modelos</h1>
            <div class="header-info">
                <span>üìù Suite: {suite_name}</span>
                <span>ü§ñ Modelos: {len(models)}</span>
                <span>üìÖ Data: {datetime.now().strftime('%d/%m/%Y %H:%M')}</span>
            </div>
        </header>

        <section class="winner">
            <h2>ü•á Vencedor</h2>
            <div class="winner-card">
                <div class="winner-name">{winner}</div>
                <div class="winner-score">{next((m['score'] for m in model_scores if m['model'] == winner), 0):.1f}%</div>
            </div>
        </section>

        <section class="ranking">
            <h2>üìä Ranking dos Modelos</h2>
            <div class="ranking-table">
                {self._generate_ranking_html(model_scores)}
            </div>
        </section>

        <section class="metrics-comparison">
            <h2>üìà Compara√ß√£o por M√©trica</h2>
            {self._generate_metrics_comparison_html(comparison_metrics.get('by_metric', {}))}
        </section>

        <section class="statistics">
            <h2>üìã Estat√≠sticas</h2>
            {self._generate_statistics_html(comparison_metrics)}
        </section>

        <footer>
            <p>Relat√≥rio gerado em {datetime.now().strftime('%d/%m/%Y √†s %H:%M:%S')} pelo Agent Core Eval System</p>
        </footer>
    </div>
</body>
</html>"""

        # Salvar arquivo
        with open(output_file, 'w', encoding='utf-8') as f:
            f.write(html_content)

        print(f"üìÑ Relat√≥rio de compara√ß√£o HTML gerado: {output_file}")
        return output_file

    def _generate_csv_single_report(self, result_data: Dict[str, Any],
                                  output_file: Optional[str] = None) -> str:
        """Gera relat√≥rio CSV para avalia√ß√£o individual."""
        if output_file is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            output_file = f"eval_report_{timestamp}.csv"

        summary = result_data.get('summary', {})
        metrics = summary.get('metrics', {})

        # Preparar dados para CSV
        rows = []

        # Linha de resumo
        rows.append([
            'RESUMO',
            summary.get('agent', 'N/A'),
            summary.get('suite', 'N/A'),
            summary.get('overall_score', 0),
            summary.get('overall_grade', 'N/A'),
            summary.get('total_tests', 0),
            summary.get('execution_time', 0)
        ])

        # Linhas de m√©tricas
        for metric_name, metric_data in metrics.items():
            rows.append([
                'METRICA',
                metric_name,
                '',
                metric_data.get('mean', 0),
                '',
                metric_data.get('passed_count', 0),
                metric_data.get('pass_rate', 0)
            ])

        # Escrever CSV
        with open(output_file, 'w', newline='', encoding='utf-8') as csvfile:
            writer = csv.writer(csvfile)
            writer.writerow(['Tipo', 'Nome/Agente', 'Suite/M√©trica', 'Score', 'Grade', 'Testes/Aprovados', 'Tempo/Taxa'])
            writer.writerows(rows)

        print(f"üìä Relat√≥rio CSV gerado: {output_file}")
        return output_file

    def _generate_markdown_single_report(self, result_data: Dict[str, Any],
                                       output_file: Optional[str] = None) -> str:
        """Gera relat√≥rio Markdown para avalia√ß√£o individual."""
        if output_file is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            output_file = f"eval_report_{timestamp}.md"

        summary = result_data.get('summary', {})
        metrics = summary.get('metrics', {})

        markdown_content = f"""# üìä Relat√≥rio de Avalia√ß√£o

**ü§ñ Agente:** {summary.get('agent', 'N/A')}
**üìù Suite:** {summary.get('suite', 'N/A')}
**üìÖ Data:** {datetime.now().strftime('%d/%m/%Y %H:%M')}

## üìà Vis√£o Geral

- **Score Geral:** {summary.get('overall_score', 0):.1f}% (Nota {summary.get('overall_grade', 'N/A')})
- **Total de Testes:** {summary.get('total_tests', 0)}
- **Tempo de Execu√ß√£o:** {summary.get('execution_time', 0):.2f}s

## üéØ M√©tricas Detalhadas

| M√©trica | Score M√©dio | Taxa de Aprova√ß√£o | Min | Max |
|---------|-------------|------------------|-----|-----|
{self._generate_metrics_table(metrics)}

## üîç An√°lise

{self._generate_markdown_analysis(metrics, summary.get('overall_score', 0))}

---
*Relat√≥rio gerado em {datetime.now().strftime('%d/%m/%Y √†s %H:%M:%S')} pelo Agent Core Eval System*
"""

        # Salvar arquivo
        with open(output_file, 'w', encoding='utf-8') as f:
            f.write(markdown_content)

        print(f"üìù Relat√≥rio Markdown gerado: {output_file}")
        return output_file

    def _generate_json_report(self, data: Dict[str, Any],
                            output_file: Optional[str] = None) -> str:
        """Gera relat√≥rio JSON (estruturado)."""
        if output_file is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            output_file = f"eval_report_{timestamp}.json"

        # Adicionar metadados do relat√≥rio
        report_data = {
            "report_metadata": {
                "generated_at": datetime.now().isoformat(),
                "generator": "Agent Core Eval System",
                "version": "1.0.0"
            },
            "data": data
        }

        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(report_data, f, indent=2, ensure_ascii=False, default=str)

        print(f"üìÑ Relat√≥rio JSON gerado: {output_file}")
        return output_file

    def _get_css_styles(self) -> str:
        """Retorna estilos CSS para os relat√≥rios HTML."""
        return """
        * { margin: 0; padding: 0; box-sizing: border-box; }

        body {
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            line-height: 1.6;
            color: #333;
            background-color: #f5f7fa;
        }

        .container {
            max-width: 1200px;
            margin: 0 auto;
            padding: 20px;
        }

        header {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            padding: 30px;
            border-radius: 10px;
            margin-bottom: 30px;
            text-align: center;
        }

        header h1 {
            font-size: 2.5em;
            margin-bottom: 15px;
        }

        .header-info {
            display: flex;
            justify-content: center;
            gap: 30px;
            font-size: 1.1em;
        }

        section {
            background: white;
            padding: 25px;
            margin-bottom: 25px;
            border-radius: 10px;
            box-shadow: 0 2px 10px rgba(0,0,0,0.1);
        }

        h2 {
            color: #2d3748;
            font-size: 1.8em;
            margin-bottom: 20px;
            border-left: 4px solid #667eea;
            padding-left: 15px;
        }

        .score-card {
            display: flex;
            align-items: center;
            justify-content: space-between;
            background: linear-gradient(135deg, #f093fb 0%, #f5576c 100%);
            padding: 30px;
            border-radius: 15px;
            color: white;
        }

        .score-value {
            font-size: 3.5em;
            font-weight: bold;
        }

        .score-grade {
            font-size: 1.2em;
            margin-top: 5px;
        }

        .grade-a { color: #48bb78; }
        .grade-b { color: #38b2ac; }
        .grade-c { color: #ed8936; }
        .grade-d { color: #e53e3e; }
        .grade-f { color: #9f1239; }

        .detail-item {
            display: flex;
            justify-content: space-between;
            margin-bottom: 10px;
            font-size: 1.1em;
        }

        .metrics-grid {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(300px, 1fr));
            gap: 20px;
        }

        .metric-card {
            border: 1px solid #e2e8f0;
            padding: 20px;
            border-radius: 8px;
            background: #f8fafc;
        }

        .metric-name {
            font-weight: bold;
            font-size: 1.2em;
            margin-bottom: 10px;
            color: #2d3748;
        }

        .metric-score {
            font-size: 2em;
            font-weight: bold;
            margin: 10px 0;
        }

        .pass { color: #48bb78; }
        .warn { color: #ed8936; }
        .fail { color: #e53e3e; }

        .progress-bar {
            background: #e2e8f0;
            height: 10px;
            border-radius: 5px;
            overflow: hidden;
            margin: 10px 0;
        }

        .progress-fill {
            height: 100%;
            border-radius: 5px;
            transition: width 0.3s ease;
        }

        footer {
            text-align: center;
            color: #666;
            margin-top: 40px;
            padding: 20px;
            border-top: 1px solid #e2e8f0;
        }
        """

    def _get_comparison_css(self) -> str:
        """CSS adicional para relat√≥rios de compara√ß√£o."""
        return """
        .winner-card {
            text-align: center;
            background: linear-gradient(135deg, #ffd700 0%, #ffed4e 100%);
            padding: 40px;
            border-radius: 15px;
            color: #2d3748;
        }

        .winner-name {
            font-size: 2.5em;
            font-weight: bold;
            margin-bottom: 10px;
        }

        .winner-score {
            font-size: 2em;
            font-weight: bold;
        }

        .ranking-table {
            overflow-x: auto;
        }

        .ranking-item {
            display: flex;
            align-items: center;
            padding: 15px;
            margin: 10px 0;
            background: #f8fafc;
            border-radius: 8px;
            border-left: 5px solid;
        }

        .rank-1 { border-left-color: #ffd700; }
        .rank-2 { border-left-color: #c0c0c0; }
        .rank-3 { border-left-color: #cd7f32; }
        .rank-other { border-left-color: #94a3b8; }

        .rank-position {
            font-size: 1.5em;
            font-weight: bold;
            margin-right: 20px;
            min-width: 50px;
        }

        .model-info {
            flex-grow: 1;
        }

        .model-name {
            font-size: 1.3em;
            font-weight: bold;
            margin-bottom: 5px;
        }

        .model-stats {
            color: #666;
        }
        """

    def _generate_metrics_html(self, metrics: Dict[str, Any]) -> str:
        """Gera HTML para exibi√ß√£o de m√©tricas."""
        html_parts = []

        for metric_name, metric_data in metrics.items():
            mean_score = metric_data.get('mean', 0)
            pass_rate = metric_data.get('pass_rate', 0)

            # Determinar classe CSS baseada no score
            if mean_score >= 80:
                score_class = "pass"
                progress_class = "pass"
            elif mean_score >= 60:
                score_class = "warn"
                progress_class = "warn"
            else:
                score_class = "fail"
                progress_class = "fail"

            html_parts.append(f"""
            <div class="metric-card">
                <div class="metric-name">{metric_name.title()}</div>
                <div class="metric-score {score_class}">{mean_score:.1f}%</div>
                <div class="progress-bar">
                    <div class="progress-fill {progress_class}" style="width: {mean_score}%"></div>
                </div>
                <div class="detail-item">
                    <span>Taxa de Aprova√ß√£o:</span>
                    <span>{pass_rate:.1f}%</span>
                </div>
                <div class="detail-item">
                    <span>Min/Max:</span>
                    <span>{metric_data.get('min', 0):.1f}% / {metric_data.get('max', 0):.1f}%</span>
                </div>
            </div>
            """)

        return "\n".join(html_parts)

    def _generate_analysis_html(self, metrics: Dict[str, Any], overall_score: float) -> str:
        """Gera an√°lise em HTML."""
        analysis_parts = []

        # An√°lise geral
        if overall_score >= 85:
            analysis_parts.append("<div class='alert success'>üéâ <strong>Excelente performance!</strong> O agente demonstrou alta qualidade em todas as m√©tricas.</div>")
        elif overall_score >= 70:
            analysis_parts.append("<div class='alert info'>üëç <strong>Boa performance.</strong> Resultados satisfat√≥rios com algumas √°reas para melhoria.</div>")
        else:
            analysis_parts.append("<div class='alert warning'>‚ö†Ô∏è <strong>Performance abaixo do esperado.</strong> V√°rias √°reas precisam de aten√ß√£o.</div>")

        # An√°lise por m√©trica
        for metric_name, metric_data in metrics.items():
            mean_score = metric_data.get('mean', 0)
            if mean_score < 70:
                analysis_parts.append(f"<div class='alert warning'>üîç <strong>{metric_name.title()}:</strong> Score baixo ({mean_score:.1f}%) - necessita melhoria.</div>")

        return "\n".join(analysis_parts) if analysis_parts else "<p>An√°lise detalhada n√£o dispon√≠vel.</p>"

    def _generate_ranking_html(self, model_scores: List[Dict[str, Any]]) -> str:
        """Gera HTML para ranking de modelos."""
        html_parts = []

        for i, model_data in enumerate(model_scores, 1):
            rank_class = f"rank-{i}" if i <= 3 else "rank-other"
            medal = "ü•á" if i == 1 else "ü•à" if i == 2 else "ü•â" if i == 3 else f"{i}¬∞"

            html_parts.append(f"""
            <div class="ranking-item {rank_class}">
                <div class="rank-position">{medal}</div>
                <div class="model-info">
                    <div class="model-name">{model_data['model']}</div>
                    <div class="model-stats">
                        Score: {model_data['score']:.1f}% |
                        Nota: {model_data['grade']} |
                        Tempo: {model_data['time']:.2f}s
                    </div>
                </div>
            </div>
            """)

        return "\n".join(html_parts)

    def _generate_metrics_comparison_html(self, metrics_comparison: Dict[str, Dict[str, float]]) -> str:
        """Gera HTML para compara√ß√£o de m√©tricas."""
        if not metrics_comparison:
            return "<p>Dados de compara√ß√£o n√£o dispon√≠veis.</p>"

        html_parts = []

        for metric_name, model_scores in metrics_comparison.items():
            sorted_models = sorted(model_scores.items(), key=lambda x: x[1], reverse=True)

            html_parts.append(f"""
            <div class="metric-comparison">
                <h3>{metric_name.title()}</h3>
                <div class="comparison-bars">
            """)

            max_score = max(model_scores.values()) if model_scores.values() else 1

            for model, score in sorted_models:
                bar_width = (score / max_score) * 100 if max_score > 0 else 0
                html_parts.append(f"""
                    <div class="comparison-bar">
                        <span class="model-label">{model}</span>
                        <div class="bar-container">
                            <div class="bar-fill" style="width: {bar_width}%"></div>
                            <span class="score-label">{score:.1f}%</span>
                        </div>
                    </div>
                """)

            html_parts.append("</div></div>")

        return "\n".join(html_parts)

    def _generate_statistics_html(self, comparison_metrics: Dict[str, Any]) -> str:
        """Gera HTML para estat√≠sticas de compara√ß√£o."""
        if not comparison_metrics:
            return "<p>Estat√≠sticas n√£o dispon√≠veis.</p>"

        return f"""
        <div class="stats-grid">
            <div class="stat-item">
                <div class="stat-value">{comparison_metrics.get('best_score', 0):.1f}%</div>
                <div class="stat-label">Melhor Score</div>
            </div>
            <div class="stat-item">
                <div class="stat-value">{comparison_metrics.get('worst_score', 0):.1f}%</div>
                <div class="stat-label">Pior Score</div>
            </div>
            <div class="stat-item">
                <div class="stat-value">{comparison_metrics.get('average_score', 0):.1f}%</div>
                <div class="stat-label">Score M√©dio</div>
            </div>
            <div class="stat-item">
                <div class="stat-value">{comparison_metrics.get('score_range', 0):.1f}%</div>
                <div class="stat-label">Diferen√ßa</div>
            </div>
        </div>
        """

    def _generate_metrics_table(self, metrics: Dict[str, Any]) -> str:
        """Gera tabela Markdown para m√©tricas."""
        rows = []
        for metric_name, metric_data in metrics.items():
            rows.append(f"| {metric_name.title()} | {metric_data.get('mean', 0):.1f}% | {metric_data.get('pass_rate', 0):.1f}% | {metric_data.get('min', 0):.1f}% | {metric_data.get('max', 0):.1f}% |")

        return "\n".join(rows) if rows else "| Nenhuma m√©trica dispon√≠vel | - | - | - | - |"

    def _generate_markdown_analysis(self, metrics: Dict[str, Any], overall_score: float) -> str:
        """Gera an√°lise em Markdown."""
        analysis_parts = []

        if overall_score >= 85:
            analysis_parts.append("### ‚úÖ An√°lise Positiva\n- Excelente performance geral")
        elif overall_score >= 70:
            analysis_parts.append("### ‚ö†Ô∏è An√°lise Moderada\n- Performance satisfat√≥ria com potencial de melhoria")
        else:
            analysis_parts.append("### ‚ùå √Åreas de Melhoria\n- Performance abaixo do esperado")

        # Identificar m√©tricas problem√°ticas
        problem_metrics = [name for name, data in metrics.items() if data.get('mean', 0) < 70]
        if problem_metrics:
            analysis_parts.append(f"\n### üîç M√©tricas que precisam de aten√ß√£o:\n" +
                                 "\n".join([f"- **{metric}**: Requer melhoria" for metric in problem_metrics]))

        return "\n".join(analysis_parts) if analysis_parts else "An√°lise n√£o dispon√≠vel."

    def _create_html_template(self):
        """Cria templates HTML b√°sicos."""
        # Template ser√° criado dinamicamente nos m√©todos de gera√ß√£o


def main():
    """Interface principal de linha de comando."""
    parser = argparse.ArgumentParser(
        description="Gerador de Relat√≥rios para Avalia√ß√µes",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Exemplos de uso:
  python utils/eval_reporter.py --input results.json --format html
  python utils/eval_reporter.py --input comparison.json --format all
  python utils/eval_reporter.py --directory eval_results --format html
        """
    )

    parser.add_argument("--input", help="Arquivo JSON com resultados")
    parser.add_argument("--directory", help="Diret√≥rio com m√∫ltiplos arquivos de resultado")
    parser.add_argument("--format", choices=["html", "json", "csv", "markdown", "all"],
                       default="html", help="Formato do relat√≥rio")
    parser.add_argument("--output", help="Nome do arquivo de sa√≠da")
    parser.add_argument("--comparison", action="store_true",
                       help="Tratar como dados de compara√ß√£o")

    args = parser.parse_args()

    if not args.input and not args.directory:
        parser.print_help()
        print("\n‚ùå Especifique --input ou --directory")
        return 1

    reporter = EvalReporter()

    try:
        if args.input:
            # Processar arquivo √∫nico
            with open(args.input, 'r', encoding='utf-8') as f:
                data = json.load(f)

            if args.format == "all":
                formats = ["html", "json", "csv", "markdown"]
            else:
                formats = [args.format]

            for format_type in formats:
                if args.comparison:
                    reporter.generate_comparison_report(data, format_type, args.output)
                else:
                    reporter.generate_single_eval_report(data, format_type, args.output)

        elif args.directory:
            # Processar diret√≥rio
            directory = Path(args.directory)
            if not directory.exists():
                print(f"‚ùå Diret√≥rio n√£o encontrado: {args.directory}")
                return 1

            json_files = list(directory.glob("*.json"))
            if not json_files:
                print(f"‚ùå Nenhum arquivo JSON encontrado em: {args.directory}")
                return 1

            print(f"üìÅ Processando {len(json_files)} arquivos...")

            for json_file in json_files:
                with open(json_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)

                output_name = f"report_{json_file.stem}.html"
                if args.comparison:
                    reporter.generate_comparison_report(data, args.format, output_name)
                else:
                    reporter.generate_single_eval_report(data, args.format, output_name)

        print("‚úÖ Relat√≥rios gerados com sucesso!")
        return 0

    except Exception as e:
        print(f"‚ùå Erro ao gerar relat√≥rios: {e}")
        return 1


if __name__ == "__main__":
    exit(main())