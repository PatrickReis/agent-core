#!/usr/bin/env python3
"""
Eval Runner - Interface CLI para execuÃ§Ã£o de avaliaÃ§Ãµes individuais.
Complementa o model_comparison.py focando em execuÃ§Ã£o de evals Ãºnicos.

Usage:
    python utils/eval_runner.py --agent local --suite basic
    python utils/eval_runner.py --custom-prompts prompts.txt --save
    python utils/eval_runner.py --quick "Pergunta 1,Pergunta 2,Pergunta 3"
"""

import os
import sys
import json
import argparse
import time
from datetime import datetime
from pathlib import Path
from typing import List, Dict, Any, Optional

# Adicionar o diretÃ³rio raiz ao path
sys.path.append(str(Path(__file__).parent.parent))

try:
    from utils.prompt_evals import EvalFramework, create_standard_eval_suite, run_quick_eval
    from utils.enhanced_logging import get_enhanced_logger
    from utils.feature_toggles import is_feature_enabled
    EVALS_AVAILABLE = True
except ImportError as e:
    print(f"âš ï¸ Sistema de evals nÃ£o disponÃ­vel: {e}")
    EVALS_AVAILABLE = False

logger = get_enhanced_logger("eval_runner") if EVALS_AVAILABLE else None


class EvalRunner:
    """Runner para execuÃ§Ã£o de avaliaÃ§Ãµes individuais."""

    def __init__(self):
        self.eval_framework = EvalFramework() if EVALS_AVAILABLE else None
        self.agent_configs = self._load_agent_configs()

    def _load_agent_configs(self) -> Dict[str, Dict[str, Any]]:
        """Carrega configuraÃ§Ãµes de agentes disponÃ­veis."""
        return {
            "local": {
                "name": "Agent Core Local",
                "description": "Agente local do Agent Core via LangGraph",
                "type": "local"
            },
            "mock": {
                "name": "Mock Agent",
                "description": "Agente simulado para testes",
                "type": "mock"
            }
        }

    def create_agent_function(self, agent_type: str):
        """Cria funÃ§Ã£o de agente baseada no tipo."""
        if agent_type == "local":
            return self._create_local_agent()
        elif agent_type == "mock":
            return self._create_mock_agent()
        else:
            raise ValueError(f"Tipo de agente nÃ£o suportado: {agent_type}")

    def _create_local_agent(self):
        """Cria funÃ§Ã£o para agente local."""
        def local_agent_function(prompt: str) -> str:
            try:
                from graphs.graph import create_agent_graph
                from providers.llm_providers import get_llm
                from langchain_core.messages import HumanMessage

                agent_graph = create_agent_graph(get_llm())
                result = agent_graph.invoke({
                    "messages": [HumanMessage(content=prompt)]
                })

                last_message = result["messages"][-1]
                return last_message.content if hasattr(last_message, 'content') else str(last_message)
            except Exception as e:
                return f"Erro no agente local: {e}"

        return local_agent_function

    def _create_mock_agent(self):
        """Cria funÃ§Ã£o para agente mock."""
        def mock_agent_function(prompt: str) -> str:
            # Simular processamento
            time.sleep(0.1)

            # Respostas baseadas no prompt
            prompt_lower = prompt.lower()

            if any(word in prompt_lower for word in ["python", "programaÃ§Ã£o", "cÃ³digo"]):
                return "Python Ã© uma linguagem de programaÃ§Ã£o interpretada, de alto nÃ­vel e de uso geral, conhecida por sua sintaxe clara e legÃ­vel."

            elif any(word in prompt_lower for word in ["clima", "temperatura", "tempo"]):
                return "O clima hoje estÃ¡ ensolarado com temperatura agradÃ¡vel de 24Â°C, ideal para atividades ao ar livre."

            elif any(word in prompt_lower for word in ["machine learning", "ml", "inteligÃªncia artificial", "ia"]):
                return "Machine Learning Ã© um subcampo da inteligÃªncia artificial que permite que sistemas aprendam e melhorem automaticamente a partir de dados sem serem explicitamente programados."

            elif any(word in prompt_lower for word in ["brasil", "capital", "brasÃ­lia"]):
                return "A capital do Brasil Ã© BrasÃ­lia, localizada na regiÃ£o Centro-Oeste do paÃ­s e inaugurada em 1960."

            elif any(word in prompt_lower for word in ["cÃ­rculo", "Ã¡rea", "matemÃ¡tica"]):
                return "A Ã¡rea de um cÃ­rculo Ã© calculada pela fÃ³rmula A = Ï€ Ã— rÂ², onde r Ã© o raio do cÃ­rculo."

            else:
                return f"Esta Ã© uma resposta informativa e contextualizada para sua pergunta sobre: {prompt[:50]}..."

        return mock_agent_function

    def run_suite_evaluation(self, agent_type: str, suite_name: str, custom_tests: Optional[List[Dict]] = None) -> Dict[str, Any]:
        """Executa avaliaÃ§Ã£o usando uma suite especÃ­fica."""
        if not EVALS_AVAILABLE:
            raise ValueError("Sistema de evals nÃ£o disponÃ­vel")

        print(f"\nğŸ§ª Executando avaliaÃ§Ã£o com suite: {suite_name}")
        print(f"ğŸ¤– Agente: {self.agent_configs[agent_type]['name']}")

        # Definir casos de teste
        if custom_tests:
            test_cases = custom_tests
        else:
            test_cases = self._get_default_suite(suite_name)

        if not test_cases:
            raise ValueError(f"Suite {suite_name} nÃ£o encontrada ou vazia")

        print(f"ğŸ“ Total de testes: {len(test_cases)}")

        # Criar suite de avaliaÃ§Ã£o
        eval_suite = create_standard_eval_suite(f"{agent_type}_{suite_name}", test_cases)

        # Criar funÃ§Ã£o do agente
        agent_function = self.create_agent_function(agent_type)

        # Executar avaliaÃ§Ã£o
        start_time = time.time()
        result = eval_suite.run(agent_function)
        execution_time = time.time() - start_time

        if result:
            print(f"\nâœ… AvaliaÃ§Ã£o concluÃ­da em {execution_time:.2f}s")
            print(f"ğŸ“Š Score geral: {result.summary['overall_score']:.1f}% (Nota: {result.summary['overall_grade']})")

            # Mostrar mÃ©tricas detalhadas
            print(f"\nğŸ“ˆ MÃ©tricas por evaluator:")
            for metric, data in result.summary['metrics'].items():
                pass_rate = data['pass_rate']
                mean_score = data['mean']
                status_icon = "âœ…" if pass_rate >= 70 else "âš ï¸" if pass_rate >= 50 else "âŒ"
                print(f"  {status_icon} {metric:12} {mean_score:6.1f}% (aprovaÃ§Ã£o: {pass_rate:5.1f}%)")

            return {
                "success": True,
                "result": result,
                "execution_time": execution_time,
                "summary": {
                    "agent": agent_type,
                    "suite": suite_name,
                    "total_tests": len(test_cases),
                    "overall_score": result.summary['overall_score'],
                    "overall_grade": result.summary['overall_grade'],
                    "metrics": result.summary['metrics']
                }
            }
        else:
            return {
                "success": False,
                "error": "AvaliaÃ§Ã£o falhou - feature possivelmente desabilitada",
                "execution_time": execution_time
            }

    def run_quick_evaluation(self, agent_type: str, prompts: List[str]) -> Dict[str, Any]:
        """Executa avaliaÃ§Ã£o rÃ¡pida com lista de prompts."""
        if not EVALS_AVAILABLE:
            raise ValueError("Sistema de evals nÃ£o disponÃ­vel")

        print(f"\nâš¡ AvaliaÃ§Ã£o rÃ¡pida")
        print(f"ğŸ¤– Agente: {self.agent_configs[agent_type]['name']}")
        print(f"ğŸ“ Prompts: {len(prompts)}")

        # Criar funÃ§Ã£o do agente
        agent_function = self.create_agent_function(agent_type)

        # Executar avaliaÃ§Ã£o rÃ¡pida
        start_time = time.time()
        metrics = run_quick_eval(prompts, agent_function)
        execution_time = time.time() - start_time

        if metrics:
            overall_score = sum(metrics.values()) / len(metrics)
            print(f"\nâœ… AvaliaÃ§Ã£o rÃ¡pida concluÃ­da em {execution_time:.2f}s")
            print(f"ğŸ“Š Score mÃ©dio: {overall_score:.1f}%")

            # Mostrar mÃ©tricas
            print(f"\nğŸ“ˆ MÃ©tricas:")
            for metric, score in metrics.items():
                status_icon = "âœ…" if score >= 70 else "âš ï¸" if score >= 50 else "âŒ"
                print(f"  {status_icon} {metric:12} {score:6.1f}%")

            return {
                "success": True,
                "metrics": metrics,
                "overall_score": overall_score,
                "execution_time": execution_time,
                "prompts_count": len(prompts)
            }
        else:
            return {
                "success": False,
                "error": "AvaliaÃ§Ã£o rÃ¡pida falhou",
                "execution_time": execution_time
            }

    def _get_default_suite(self, suite_name: str) -> List[Dict[str, Any]]:
        """Retorna casos de teste para suites padrÃ£o."""
        suites = {
            "basic": [
                {"prompt": "O que Ã© Python?", "expected": "Python Ã© uma linguagem de programaÃ§Ã£o"},
                {"prompt": "Qual a capital do Brasil?", "expected": "BrasÃ­lia"},
                {"prompt": "Como calcular a Ã¡rea de um cÃ­rculo?"},
                {"prompt": "Explique o conceito de IA em uma frase"}
            ],

            "reasoning": [
                {"prompt": "Por que o cÃ©u Ã© azul? Explique de forma simples."},
                {"prompt": "Se chove quando hÃ¡ nuvens escuras, e hÃ¡ nuvens escuras agora, vai chover?"},
                {"prompt": "Compare os prÃ³s e contras de energia solar"},
                {"prompt": "Qual a relaÃ§Ã£o entre causa e efeito na programaÃ§Ã£o?"}
            ],

            "knowledge": [
                {"prompt": "Explique machine learning em duas frases"},
                {"prompt": "Qual a diferenÃ§a entre HTTP e HTTPS?"},
                {"prompt": "Como funciona a internet?"},
                {"prompt": "O que sÃ£o APIs e para que servem?"}
            ],

            "performance": [
                {"prompt": "Resposta rÃ¡pida: 2+2"},
                {"prompt": "Uma palavra: capital da FranÃ§a"},
                {"prompt": "Sim ou nÃ£o: Python Ã© compilado?"},
                {"prompt": "Nome de uma cor primÃ¡ria"}
            ],

            "comprehensive": [
                # Mistura de todos os tipos
                {"prompt": "O que Ã© Python?", "expected": "Python Ã© uma linguagem de programaÃ§Ã£o"},
                {"prompt": "Por que o cÃ©u Ã© azul?"},
                {"prompt": "Explique machine learning brevemente"},
                {"prompt": "Capital do Brasil?", "expected": "BrasÃ­lia"},
                {"prompt": "Como calcular Ã¡rea de cÃ­rculo?"},
                {"prompt": "DiferenÃ§a entre HTTP e HTTPS?"},
                {"prompt": "2+2 = ?", "expected": "4"},
                {"prompt": "Python Ã© compilado?", "expected": "NÃ£o"}
            ]
        }

        return suites.get(suite_name, [])

    def load_prompts_from_file(self, file_path: str) -> List[str]:
        """Carrega prompts de arquivo texto."""
        if not Path(file_path).exists():
            raise FileNotFoundError(f"Arquivo nÃ£o encontrado: {file_path}")

        prompts = []
        with open(file_path, 'r', encoding='utf-8') as f:
            for line in f:
                line = line.strip()
                if line and not line.startswith('#'):  # Ignorar comentÃ¡rios
                    prompts.append(line)

        return prompts

    def save_results(self, results: Dict[str, Any], output_file: Optional[str] = None) -> str:
        """Salva resultados em arquivo JSON."""
        if output_file is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            output_file = f"eval_results_{timestamp}.json"

        # Preparar dados para serializaÃ§Ã£o
        serializable_results = {}
        for key, value in results.items():
            if hasattr(value, 'to_dict'):
                serializable_results[key] = value.to_dict()
            elif hasattr(value, '__dict__'):
                serializable_results[key] = value.__dict__
            else:
                serializable_results[key] = value

        # Salvar arquivo
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(serializable_results, f, indent=2, ensure_ascii=False, default=str)

        print(f"ğŸ’¾ Resultados salvos em: {output_file}")
        return output_file

    def list_available_suites(self):
        """Lista suites de teste disponÃ­veis."""
        print(f"\nğŸ“ SUITES DE TESTE DISPONÃVEIS:")
        print("-" * 50)

        suites_info = {
            "basic": "Testes bÃ¡sicos de conhecimento geral",
            "reasoning": "Testes de raciocÃ­nio e lÃ³gica",
            "knowledge": "Testes de conhecimento tÃ©cnico",
            "performance": "Testes de velocidade de resposta",
            "comprehensive": "Suite completa com todos os tipos"
        }

        for name, description in suites_info.items():
            test_count = len(self._get_default_suite(name))
            print(f"ğŸ“‹ {name:<14} - {description} ({test_count} testes)")

    def list_available_agents(self):
        """Lista agentes disponÃ­veis."""
        print(f"\nğŸ¤– AGENTES DISPONÃVEIS:")
        print("-" * 50)

        for name, config in self.agent_configs.items():
            print(f"ğŸ¤– {name:<10} - {config['description']}")


def main():
    """Interface principal de linha de comando."""
    parser = argparse.ArgumentParser(
        description="Eval Runner - ExecuÃ§Ã£o de avaliaÃ§Ãµes individuais",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Exemplos de uso:
  python utils/eval_runner.py --agent local --suite basic
  python utils/eval_runner.py --agent mock --suite reasoning --save
  python utils/eval_runner.py --agent local --quick "O que Ã© Python?,Capital do Brasil?"
  python utils/eval_runner.py --agent mock --custom-prompts my_prompts.txt
  python utils/eval_runner.py --list-suites
  python utils/eval_runner.py --list-agents
        """
    )

    parser.add_argument("--agent", choices=["local", "mock"], default="mock",
                       help="Tipo de agente para avaliar")
    parser.add_argument("--suite", help="Suite de teste padrÃ£o para usar")
    parser.add_argument("--quick", help="Prompts separados por vÃ­rgula para avaliaÃ§Ã£o rÃ¡pida")
    parser.add_argument("--custom-prompts", help="Arquivo com prompts personalizados")
    parser.add_argument("--save", action="store_true", help="Salvar resultados em arquivo")
    parser.add_argument("--output", help="Arquivo de saÃ­da para resultados")
    parser.add_argument("--list-suites", action="store_true", help="Listar suites disponÃ­veis")
    parser.add_argument("--list-agents", action="store_true", help="Listar agentes disponÃ­veis")

    args = parser.parse_args()

    if not EVALS_AVAILABLE:
        print("âŒ Sistema de evals nÃ£o estÃ¡ disponÃ­vel")
        return 1

    # Criar runner
    runner = EvalRunner()

    # Listar suites
    if args.list_suites:
        runner.list_available_suites()
        return 0

    # Listar agentes
    if args.list_agents:
        runner.list_available_agents()
        return 0

    # Validar que pelo menos uma opÃ§Ã£o de teste foi especificada
    if not any([args.suite, args.quick, args.custom_prompts]):
        parser.print_help()
        print("\nâŒ Especifique --suite, --quick ou --custom-prompts")
        return 1

    try:
        results = None

        # AvaliaÃ§Ã£o rÃ¡pida
        if args.quick:
            prompts = [p.strip() for p in args.quick.split(',')]
            results = runner.run_quick_evaluation(args.agent, prompts)

        # Prompts de arquivo
        elif args.custom_prompts:
            prompts = runner.load_prompts_from_file(args.custom_prompts)
            print(f"ğŸ“ Carregados {len(prompts)} prompts de {args.custom_prompts}")
            results = runner.run_quick_evaluation(args.agent, prompts)

        # Suite padrÃ£o
        elif args.suite:
            results = runner.run_suite_evaluation(args.agent, args.suite)

        # Salvar resultados se solicitado
        if results and args.save:
            runner.save_results(results, args.output)

        return 0 if results and results.get('success', False) else 1

    except Exception as e:
        print(f"âŒ Erro durante execuÃ§Ã£o: {e}")
        return 1


if __name__ == "__main__":
    exit(main())