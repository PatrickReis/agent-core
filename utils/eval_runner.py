#!/usr/bin/env python3
"""
Eval Runner - Interface CLI para execu√ß√£o de avalia√ß√µes individuais.
Complementa o model_comparison.py focando em execu√ß√£o de evals √∫nicos.

Usage:
    python utils/eval_runner.py --agent local --suite basic
    python utils/eval_runner.py --custom-prompts prompts.txt --save
    python utils/eval_runner.py --quick "Pergunta 1,Pergunta 2,Pergunta 3"
"""

import os
import sys
import json
import argparse
import time
from datetime import datetime
from pathlib import Path
from typing import List, Dict, Any, Optional

# Adicionar o diret√≥rio raiz ao path
sys.path.append(str(Path(__file__).parent.parent))

try:
    from utils.prompt_evals import EvalFramework, create_standard_eval_suite, run_quick_eval
    from utils.enhanced_logging import get_enhanced_logger
    from utils.feature_toggles import is_feature_enabled
    EVALS_AVAILABLE = True
except ImportError as e:
    print(f"‚ö†Ô∏è Sistema de evals n√£o dispon√≠vel: {e}")
    EVALS_AVAILABLE = False

logger = get_enhanced_logger("eval_runner") if EVALS_AVAILABLE else None


class EvalRunner:
    """Runner para execu√ß√£o de avalia√ß√µes individuais."""

    def __init__(self):
        self.eval_framework = EvalFramework() if EVALS_AVAILABLE else None
        self.agent_configs = self._load_agent_configs()

    def _load_agent_configs(self) -> Dict[str, Dict[str, Any]]:
        """Carrega configura√ß√µes de agentes dispon√≠veis."""
        return {
            "local": {
                "name": "Agent Core Local",
                "description": "Agente local do Agent Core via LangGraph",
                "type": "local"
            },
            "mock": {
                "name": "Mock Agent",
                "description": "Agente simulado para testes",
                "type": "mock"
            }
        }

    def create_agent_function(self, agent_type: str):
        """Cria fun√ß√£o de agente baseada no tipo."""
        if agent_type == "local":
            return self._create_local_agent()
        elif agent_type == "mock":
            return self._create_mock_agent()
        else:
            raise ValueError(f"Tipo de agente n√£o suportado: {agent_type}")

    def _create_local_agent(self):
        """Cria fun√ß√£o para agente local."""
        def local_agent_function(prompt: str) -> str:
            try:
                from graphs.graph import create_agent_graph
                from providers.llm_providers import get_llm
                from langchain_core.messages import HumanMessage

                agent_graph = create_agent_graph(get_llm())
                result = agent_graph.invoke({
                    "messages": [HumanMessage(content=prompt)]
                })

                last_message = result["messages"][-1]
                return last_message.content if hasattr(last_message, 'content') else str(last_message)
            except Exception as e:
                return f"Erro no agente local: {e}"

        return local_agent_function

    def _create_mock_agent(self):
        """Cria fun√ß√£o para agente mock."""
        def mock_agent_function(prompt: str) -> str:
            # Simular processamento
            time.sleep(0.1)

            # Respostas baseadas no prompt
            prompt_lower = prompt.lower()

            if any(word in prompt_lower for word in ["python", "programa√ß√£o", "c√≥digo"]):
                return "Python √© uma linguagem de programa√ß√£o interpretada, de alto n√≠vel e de uso geral, conhecida por sua sintaxe clara e leg√≠vel."

            elif any(word in prompt_lower for word in ["clima", "temperatura", "tempo"]):
                return "O clima hoje est√° ensolarado com temperatura agrad√°vel de 24¬∞C, ideal para atividades ao ar livre."

            elif any(word in prompt_lower for word in ["machine learning", "ml", "intelig√™ncia artificial", "ia"]):
                return "Machine Learning √© um subcampo da intelig√™ncia artificial que permite que sistemas aprendam e melhorem automaticamente a partir de dados sem serem explicitamente programados."

            elif any(word in prompt_lower for word in ["brasil", "capital", "bras√≠lia"]):
                return "A capital do Brasil √© Bras√≠lia, localizada na regi√£o Centro-Oeste do pa√≠s e inaugurada em 1960."

            elif any(word in prompt_lower for word in ["c√≠rculo", "√°rea", "matem√°tica"]):
                return "A √°rea de um c√≠rculo √© calculada pela f√≥rmula A = œÄ √ó r¬≤, onde r √© o raio do c√≠rculo."

            else:
                return f"Esta √© uma resposta informativa e contextualizada para sua pergunta sobre: {prompt[:50]}..."

        return mock_agent_function

    def run_suite_evaluation(self, agent_type: str, suite_name: str, custom_tests: Optional[List[Dict]] = None) -> Dict[str, Any]:
        """Executa avalia√ß√£o usando uma suite espec√≠fica."""
        if not EVALS_AVAILABLE:
            raise ValueError("Sistema de evals n√£o dispon√≠vel")

        print(f"\nüß™ Executando avalia√ß√£o com suite: {suite_name}")
        print(f"ü§ñ Agente: {self.agent_configs[agent_type]['name']}")

        # Definir casos de teste
        if custom_tests:
            test_cases = custom_tests
        else:
            test_cases = self._get_default_suite(suite_name)

        if not test_cases:
            raise ValueError(f"Suite {suite_name} n√£o encontrada ou vazia")

        print(f"üìù Total de testes: {len(test_cases)}")

        # Criar suite de avalia√ß√£o
        eval_suite = create_standard_eval_suite(f"{agent_type}_{suite_name}", test_cases)

        # Criar fun√ß√£o do agente
        agent_function = self.create_agent_function(agent_type)

        # Executar avalia√ß√£o
        start_time = time.time()
        result = eval_suite.run(agent_function)
        execution_time = time.time() - start_time

        if result:
            print(f"\n‚úÖ Avalia√ß√£o conclu√≠da em {execution_time:.2f}s")
            print(f"üìä Score geral: {result.summary['overall_score']:.1f}% (Nota: {result.summary['overall_grade']})")

            # Mostrar m√©tricas detalhadas
            print(f"\nüìà M√©tricas por evaluator:")
            for metric, data in result.summary['metrics'].items():
                pass_rate = data['pass_rate']
                mean_score = data['mean']
                status_icon = "‚úÖ" if pass_rate >= 70 else "‚ö†Ô∏è" if pass_rate >= 50 else "‚ùå"
                print(f"  {status_icon} {metric:12} {mean_score:6.1f}% (aprova√ß√£o: {pass_rate:5.1f}%)")

            return {
                "success": True,
                "result": result,
                "execution_time": execution_time,
                "summary": {
                    "agent": agent_type,
                    "suite": suite_name,
                    "total_tests": len(test_cases),
                    "overall_score": result.summary['overall_score'],
                    "overall_grade": result.summary['overall_grade'],
                    "metrics": result.summary['metrics']
                }
            }
        else:
            return {
                "success": False,
                "error": "Avalia√ß√£o falhou - feature possivelmente desabilitada",
                "execution_time": execution_time
            }

    def run_quick_evaluation(self, agent_type: str, prompts: List[str]) -> Dict[str, Any]:
        """Executa avalia√ß√£o r√°pida com lista de prompts."""
        if not EVALS_AVAILABLE:
            raise ValueError("Sistema de evals n√£o dispon√≠vel")

        print(f"\n‚ö° Avalia√ß√£o r√°pida")
        print(f"ü§ñ Agente: {self.agent_configs[agent_type]['name']}")
        print(f"üìù Prompts: {len(prompts)}")

        # Criar fun√ß√£o do agente
        agent_function = self.create_agent_function(agent_type)

        # Executar avalia√ß√£o r√°pida
        start_time = time.time()
        metrics = run_quick_eval(prompts, agent_function)
        execution_time = time.time() - start_time

        if metrics:
            overall_score = sum(metrics.values()) / len(metrics)
            print(f"\n‚úÖ Avalia√ß√£o r√°pida conclu√≠da em {execution_time:.2f}s")
            print(f"üìä Score m√©dio: {overall_score:.1f}%")

            # Mostrar m√©tricas
            print(f"\nüìà M√©tricas:")
            for metric, score in metrics.items():
                status_icon = "‚úÖ" if score >= 70 else "‚ö†Ô∏è" if score >= 50 else "‚ùå"
                print(f"  {status_icon} {metric:12} {score:6.1f}%")

            return {
                "success": True,
                "metrics": metrics,
                "overall_score": overall_score,
                "execution_time": execution_time,
                "prompts_count": len(prompts)
            }
        else:
            return {
                "success": False,
                "error": "Avalia√ß√£o r√°pida falhou",
                "execution_time": execution_time
            }

    def _get_default_suite(self, suite_name: str) -> List[Dict[str, Any]]:
        """Retorna casos de teste para suites padr√£o."""
        suites = {
            "basic": [
                {"prompt": "O que √© Python?", "expected": "Python √© uma linguagem de programa√ß√£o"},
                {"prompt": "Qual a capital do Brasil?", "expected": "Bras√≠lia"},
                {"prompt": "Como calcular a √°rea de um c√≠rculo?"},
                {"prompt": "Explique o conceito de IA em uma frase"}
            ],

            "reasoning": [
                {"prompt": "Por que o c√©u √© azul? Explique de forma simples."},
                {"prompt": "Se chove quando h√° nuvens escuras, e h√° nuvens escuras agora, vai chover?"},
                {"prompt": "Compare os pr√≥s e contras de energia solar"},
                {"prompt": "Qual a rela√ß√£o entre causa e efeito na programa√ß√£o?"}
            ],

            "knowledge": [
                {"prompt": "Explique machine learning em duas frases"},
                {"prompt": "Qual a diferen√ßa entre HTTP e HTTPS?"},
                {"prompt": "Como funciona a internet?"},
                {"prompt": "O que s√£o APIs e para que servem?"}
            ],

            "performance": [
                {"prompt": "Resposta r√°pida: 2+2"},
                {"prompt": "Uma palavra: capital da Fran√ßa"},
                {"prompt": "Sim ou n√£o: Python √© compilado?"},
                {"prompt": "Nome de uma cor prim√°ria"}
            ],

            "comprehensive": [
                # Mistura de todos os tipos
                {"prompt": "O que √© Python?", "expected": "Python √© uma linguagem de programa√ß√£o"},
                {"prompt": "Por que o c√©u √© azul?"},
                {"prompt": "Explique machine learning brevemente"},
                {"prompt": "Capital do Brasil?", "expected": "Bras√≠lia"},
                {"prompt": "Como calcular √°rea de c√≠rculo?"},
                {"prompt": "Diferen√ßa entre HTTP e HTTPS?"},
                {"prompt": "2+2 = ?", "expected": "4"},
                {"prompt": "Python √© compilado?", "expected": "N√£o"}
            ]
        }

        return suites.get(suite_name, [])

    def load_prompts_from_file(self, file_path: str) -> List[str]:
        """Carrega prompts de arquivo texto."""
        if not Path(file_path).exists():
            raise FileNotFoundError(f"Arquivo n√£o encontrado: {file_path}")

        prompts = []
        with open(file_path, 'r', encoding='utf-8') as f:
            for line in f:
                line = line.strip()
                if line and not line.startswith('#'):  # Ignorar coment√°rios
                    prompts.append(line)

        return prompts

    def save_results(self, results: Dict[str, Any], output_file: Optional[str] = None) -> str:
        """Salva resultados em arquivo JSON."""
        if output_file is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            output_file = f"eval_results_{timestamp}.json"

        # Preparar dados para serializa√ß√£o
        serializable_results = {}
        for key, value in results.items():
            if hasattr(value, 'to_dict'):
                serializable_results[key] = value.to_dict()
            elif hasattr(value, '__dict__'):
                serializable_results[key] = value.__dict__
            else:
                serializable_results[key] = value

        # Salvar arquivo
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(serializable_results, f, indent=2, ensure_ascii=False, default=str)

        print(f"üíæ Resultados salvos em: {output_file}")
        return output_file

    def list_available_suites(self):
        """Lista suites de teste dispon√≠veis."""
        print(f"\nüìù SUITES DE TESTE DISPON√çVEIS:")
        print("-" * 50)

        suites_info = {
            "basic": "Testes b√°sicos de conhecimento geral",
            "reasoning": "Testes de racioc√≠nio e l√≥gica",
            "knowledge": "Testes de conhecimento t√©cnico",
            "performance": "Testes de velocidade de resposta",
            "comprehensive": "Suite completa com todos os tipos"
        }

        for name, description in suites_info.items():
            test_count = len(self._get_default_suite(name))
            print(f"üìã {name:<14} - {description} ({test_count} testes)")

    def list_available_agents(self):
        """Lista agentes dispon√≠veis."""
        print(f"\nü§ñ AGENTES DISPON√çVEIS:")
        print("-" * 50)

        for name, config in self.agent_configs.items():
            print(f"ü§ñ {name:<10} - {config['description']}")


def main():
    """Interface principal de linha de comando."""
    parser = argparse.ArgumentParser(
        description="Eval Runner - Execu√ß√£o de avalia√ß√µes individuais",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Exemplos de uso:
  python utils/eval_runner.py --agent local --suite basic
  python utils/eval_runner.py --agent mock --suite reasoning --save
  python utils/eval_runner.py --agent local --quick "O que √© Python?,Capital do Brasil?"
  python utils/eval_runner.py --agent mock --custom-prompts my_prompts.txt
  python utils/eval_runner.py --list-suites
  python utils/eval_runner.py --list-agents
        """
    )

    parser.add_argument("--agent", choices=["local", "mock"], default="mock",
                       help="Tipo de agente para avaliar")
    parser.add_argument("--suite", help="Suite de teste padr√£o para usar")
    parser.add_argument("--quick", help="Prompts separados por v√≠rgula para avalia√ß√£o r√°pida")
    parser.add_argument("--custom-prompts", help="Arquivo com prompts personalizados")
    parser.add_argument("--save", action="store_true", help="Salvar resultados em arquivo")
    parser.add_argument("--output", help="Arquivo de sa√≠da para resultados")
    parser.add_argument("--list-suites", action="store_true", help="Listar suites dispon√≠veis")
    parser.add_argument("--list-agents", action="store_true", help="Listar agentes dispon√≠veis")

    args = parser.parse_args()

    if not EVALS_AVAILABLE:
        print("‚ùå Sistema de evals n√£o est√° dispon√≠vel")
        return 1

    # Criar runner
    runner = EvalRunner()

    # Listar suites
    if args.list_suites:
        runner.list_available_suites()
        return 0

    # Listar agentes
    if args.list_agents:
        runner.list_available_agents()
        return 0

    # Validar que pelo menos uma op√ß√£o de teste foi especificada
    if not any([args.suite, args.quick, args.custom_prompts]):
        parser.print_help()
        print("\n‚ùå Especifique --suite, --quick ou --custom-prompts")
        return 1

    try:
        results = None

        # Avalia√ß√£o r√°pida
        if args.quick:
            prompts = [p.strip() for p in args.quick.split(',')]
            results = runner.run_quick_evaluation(args.agent, prompts)

        # Prompts de arquivo
        elif args.custom_prompts:
            prompts = runner.load_prompts_from_file(args.custom_prompts)
            print(f"üìÅ Carregados {len(prompts)} prompts de {args.custom_prompts}")
            results = runner.run_quick_evaluation(args.agent, prompts)

        # Suite padr√£o
        elif args.suite:
            results = runner.run_suite_evaluation(args.agent, args.suite)

        # Salvar resultados se solicitado
        if results and args.save:
            runner.save_results(results, args.output)

        return 0 if results and results.get('success', False) else 1

    except Exception as e:
        print(f"‚ùå Erro durante execu√ß√£o: {e}")
        return 1


if __name__ == "__main__":
    exit(main())