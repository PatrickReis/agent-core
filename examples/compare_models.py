#!/usr/bin/env python3
"""
Exemplos de compara√ß√£o de modelos espec√≠ficos.
Demonstra como comparar Llama vs Gemini e modelos Ollama.
"""

import os
import sys
from pathlib import Path

# Adicionar o diret√≥rio raiz ao path
sys.path.append(str(Path(__file__).parent.parent))

from utils.model_comparison import ModelComparisonFramework


def example_llama_vs_gemini():
    """Exemplo: Comparar Llama 3 (local) vs Gemini 1.5 (cloud)."""
    print("\nüî• EXEMPLO 1: LLAMA 3 vs GEMINI 1.5")
    print("=" * 50)

    framework = ModelComparisonFramework()

    # Verificar se modelos est√£o dispon√≠veis
    print("üìã Verificando disponibilidade dos modelos...")

    # Verificar Gemini API Key
    gemini_key = os.getenv("GEMINI_API_KEY")
    if not gemini_key or gemini_key == "AIzaSyBTTv0AW2t3Zwth742xwDo2Yyw3f7_QytQ":
        print("‚ö†Ô∏è  GEMINI_API_KEY n√£o configurada ou usando valor de exemplo")
        print("   Configure uma API key v√°lida no .env para usar Gemini")
        return False

    # Verificar Ollama
    try:
        import requests
        response = requests.get("http://localhost:11434/api/tags", timeout=5)
        if response.status_code == 200:
            models = response.json().get("models", [])
            model_names = [m["name"] for m in models]
            if "llama3:latest" not in model_names:
                print("‚ö†Ô∏è  Modelo llama3:latest n√£o encontrado no Ollama")
                print("   Execute: ollama pull llama3")
                return False
            print("‚úÖ Ollama conectado e llama3:latest dispon√≠vel")
        else:
            print("‚ùå Ollama n√£o est√° respondendo em localhost:11434")
            return False
    except Exception as e:
        print(f"‚ùå Erro ao conectar com Ollama: {e}")
        return False

    # Executar compara√ß√£o
    try:
        print("\nüöÄ Iniciando compara√ß√£o...")
        result = framework.compare_models(
            ["llama3_ollama", "gemini_15_flash"],
            "basic"
        )

        # Mostrar relat√≥rio
        framework.print_comparison_report(result)

        # Salvar resultado
        result.save_to_file("comparison_llama_vs_gemini.json")

        return True

    except Exception as e:
        print(f"‚ùå Erro durante compara√ß√£o: {e}")
        return False


def example_ollama_models():
    """Exemplo: Comparar modelos Ollama (Llama 3 vs GPT-OSS)."""
    print("\nüî• EXEMPLO 2: MODELOS OLLAMA (LLAMA 3 vs GPT-OSS)")
    print("=" * 60)

    framework = ModelComparisonFramework()

    # Verificar se modelos est√£o dispon√≠veis no Ollama
    print("üìã Verificando modelos no Ollama...")

    try:
        import requests
        response = requests.get("http://localhost:11434/api/tags", timeout=5)
        if response.status_code == 200:
            models = response.json().get("models", [])
            model_names = [m["name"] for m in models]

            print(f"üîç Modelos encontrados no Ollama:")
            for name in model_names:
                print(f"   ‚Ä¢ {name}")

            # Verificar se os modelos necess√°rios est√£o dispon√≠veis
            required_models = ["llama3:latest", "gpt-oss:latest"]
            available = []

            for model in required_models:
                if model in model_names:
                    print(f"‚úÖ {model} - Dispon√≠vel")
                    available.append(model)
                else:
                    print(f"‚ùå {model} - N√ÉO encontrado")
                    print(f"   Execute: ollama pull {model.split(':')[0]}")

            if len(available) < 2:
                print("\n‚ö†Ô∏è  Pelo menos 2 modelos s√£o necess√°rios para compara√ß√£o")
                return False

        else:
            print("‚ùå Ollama n√£o est√° respondendo")
            return False

    except Exception as e:
        print(f"‚ùå Erro ao conectar com Ollama: {e}")
        return False

    # Executar compara√ß√£o
    try:
        print("\nüöÄ Iniciando compara√ß√£o entre modelos Ollama...")
        result = framework.compare_models(
            ["llama3_ollama", "gpt_oss_ollama"],
            "reasoning"
        )

        # Mostrar relat√≥rio
        framework.print_comparison_report(result)

        # Salvar resultado
        result.save_to_file("comparison_ollama_models.json")

        return True

    except Exception as e:
        print(f"‚ùå Erro durante compara√ß√£o: {e}")
        return False


def check_requirements():
    """Verifica depend√™ncias necess√°rias."""
    print("üîç VERIFICANDO DEPEND√äNCIAS")
    print("=" * 30)

    # Verificar Python packages
    required_packages = [
        ("requests", "Para comunica√ß√£o com Ollama"),
        ("google-generativeai", "Para uso do Gemini (opcional)"),
    ]

    missing_packages = []

    for package, description in required_packages:
        try:
            __import__(package.replace("-", "_"))
            print(f"‚úÖ {package} - {description}")
        except ImportError:
            print(f"‚ùå {package} - {description}")
            missing_packages.append(package)

    if missing_packages:
        print(f"\nüì¶ Instale os packages necess√°rios:")
        print(f"pip install {' '.join(missing_packages)}")
        return False

    # Verificar servi√ßos
    print(f"\nüîç VERIFICANDO SERVI√áOS")
    print("-" * 25)

    # Ollama
    try:
        import requests
        response = requests.get("http://localhost:11434/api/version", timeout=3)
        if response.status_code == 200:
            version_info = response.json()
            print(f"‚úÖ Ollama v{version_info.get('version', 'unknown')} - Rodando")
        else:
            print("‚ùå Ollama - N√£o est√° respondendo")
            return False
    except Exception:
        print("‚ùå Ollama - N√£o est√° rodando ou n√£o est√° instalado")
        print("   Instale: https://ollama.ai/")
        return False

    # API Keys
    print(f"\nüîë VERIFICANDO API KEYS")
    print("-" * 22)

    gemini_key = os.getenv("GEMINI_API_KEY")
    if gemini_key and gemini_key != "AIzaSyBTTv0AW2t3Zwth742xwDo2Yyw3f7_QytQ":
        print("‚úÖ GEMINI_API_KEY - Configurada")
    else:
        print("‚ö†Ô∏è  GEMINI_API_KEY - N√£o configurada ou usando valor de exemplo")

    return True


def interactive_menu():
    """Menu interativo para sele√ß√£o de exemplos."""
    while True:
        print("\nüéØ MENU DE COMPARA√á√ÉO DE MODELOS")
        print("=" * 35)
        print("1. üî• Llama 3 vs Gemini 1.5")
        print("2. üî• Modelos Ollama (Llama vs GPT-OSS)")
        print("3. üîç Verificar depend√™ncias")
        print("4. üìã Listar modelos dispon√≠veis")
        print("5. ‚ùå Sair")

        try:
            choice = input("\n‚û§ Escolha uma op√ß√£o (1-5): ").strip()

            if choice == "1":
                example_llama_vs_gemini()
            elif choice == "2":
                example_ollama_models()
            elif choice == "3":
                check_requirements()
            elif choice == "4":
                framework = ModelComparisonFramework()
                framework.list_available_models()
            elif choice == "5":
                print("üëã Saindo...")
                break
            else:
                print("‚ùå Op√ß√£o inv√°lida")

            input("\n‚è∏Ô∏è  Pressione Enter para continuar...")

        except KeyboardInterrupt:
            print("\nüëã Saindo...")
            break
        except Exception as e:
            print(f"‚ùå Erro: {e}")


def main():
    """Fun√ß√£o principal."""
    print("üöÄ SISTEMA DE COMPARA√á√ÉO DE MODELOS")
    print("Agent Core - Eval System")
    print("=" * 40)

    # Verificar depend√™ncias b√°sicas
    if not check_requirements():
        print("\n‚ùå Depend√™ncias n√£o satisfeitas. Corrija os problemas acima.")
        return 1

    # Menu interativo
    interactive_menu()

    return 0


if __name__ == "__main__":
    exit(main())